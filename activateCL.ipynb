{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import Colours\n",
    "import logging, argparse\n",
    "import numpy as np\n",
    "import glob, os, shutil\n",
    "import random, json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Subset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP,XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMultipleChoice,\n",
    "    RobertaTokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForMultipleChoice,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "# from transformers import WarmupLinearSchedule\n",
    "from utils_multiple_choice import convert_examples_to_features, processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum(\n",
    "    (tuple(conf.keys()) for conf in (BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP)), ()\n",
    ")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForMultipleChoice, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForMultipleChoice, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n",
    "}\n",
    "\n",
    "\n",
    "def select_field(features, field):\n",
    "    return [[choice[field] for choice in feature.choices_features] for feature in features]\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def dataset_with_indices(cls):\n",
    "    \"\"\"\n",
    "    Modifies the given Dataset class to return a tuple data, target, index\n",
    "    instead of just data, target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = cls.__getitem__(self, index)\n",
    "        return data + (index,)\n",
    "\n",
    "    return type(cls.__name__, (cls,), {\n",
    "        '__getitem__': __getitem__,\n",
    "    })\n",
    "\n",
    "\n",
    "IndexedTensorDataset = dataset_with_indices(TensorDataset)\n",
    "\n",
    "\n",
    "class SPLLoss(torch.nn.NLLLoss):\n",
    "    def __init__(self, *args, device=torch.device(\"cpu\"), n_samples=0, warmup_steps=500, **kwargs):\n",
    "        super(SPLLoss, self).__init__(*args, **kwargs)\n",
    "        self.threshold = 0.5\n",
    "        self.growing_factor = 1.3\n",
    "        self.v = torch.zeros(n_samples).int().to(device)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, index: torch.Tensor, n_steps) -> torch.Tensor:\n",
    "        super_loss = torch.nn.functional.nll_loss(torch.log_softmax(input, dim=-1), target, reduction=\"none\")\n",
    "\n",
    "        # if n_steps <= self.warmup_steps:\n",
    "        #    return super_loss.mean()\n",
    "        # else:\n",
    "        v = self.spl_loss(super_loss)\n",
    "        self.v[index] = v\n",
    "        return (super_loss * v.float()).mean()\n",
    "\n",
    "    def increase_threshold(self):\n",
    "        self.threshold *= self.growing_factor\n",
    "\n",
    "    def spl_loss(self, super_loss):\n",
    "        v = super_loss < self.threshold\n",
    "        return v.int()\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = self.v.detach().cpu().numpy()\n",
    "        np.save('weights.npy', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = '5e-6'\n",
    "batch_size = 8\n",
    "n_epochs = 4\n",
    "n_epochs = str(n_epochs)\n",
    "if batch_size == 16:\n",
    "    gradient_acc_steps = \"8\"\n",
    "elif batch_size == 8:\n",
    "    gradient_acc_steps = \"4\"\n",
    "else:\n",
    "    raise ValueError\n",
    "warmup_steps = str(int(0.06 * 30000 / batch_size))\n",
    "\n",
    "args_list = ['--model_type', 'roberta',\n",
    "            '--task_name', 'cosmosqa',\n",
    "            '--model_name_or_path', 'roberta-large',\n",
    "            '--train_file', 'train.jsonl',\n",
    "            '--eval_file', 'valid.jsonl',\n",
    "            '--data_dir', '../data/cosmosqa/',\n",
    "            '--output_dir', './baselines/cosmosqa-roberta-large/bayes-'  + learning_rate + '-' + n_epochs + '-' + str(batch_size),\n",
    "            '--logging_steps', '200',\n",
    "            '--do_train', '--do_eval',\n",
    "            '--num_train_epochs', n_epochs,\n",
    "            '--max_seq_length', '128',\n",
    "            '--save_steps', '1000',\n",
    "            '--overwrite_output',\n",
    "            '--per_gpu_eval_batch_size', '8',\n",
    "            '--per_gpu_train_batch_size', '2',\n",
    "            '--gradient_accumulation_steps', gradient_acc_steps,\n",
    "            '--warmup_steps', warmup_steps,\n",
    "            '--learning_rate', learning_rate,\n",
    "            '--weight_decay', '0.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta_train_and_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/23/2023 01:53:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "args.task_name = args.task_name.lower()\n",
    "if args.task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "processor = processors[args.task_name]()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/23/2023 01:54:12 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='../data/cosmosqa/', model_type='roberta', model_name_or_path='roberta-large', task_name='cosmosqa', output_dir='./baselines/cosmosqa-roberta-large/bayes-5e-6-4-8', config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, train_file='train.jsonl', eval_file='valid.jsonl', do_train=True, do_eval=True, do_test=False, curriculum_learning=False, starting_percent=0.3, increase_factor=1.1, step_length=750, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=5e-06, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=225, logging_steps=200, save_steps=1000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "best_steps = 0\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load_and_cache_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = False\n",
    "test = False\n",
    "cached_mode = \"train\"\n",
    "task = args.task_name\n",
    "cached_features_file = os.path.join(\n",
    "    args.output_dir,\n",
    "    \"cached_{}_{}_{}_{}\".format(\n",
    "        cached_mode,\n",
    "        list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "        str(args.max_seq_length),\n",
    "        str(task),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/23/2023 02:05:08 - INFO - __main__ -   Creating features from dataset file at ../data/cosmosqa/\n",
      "10/23/2023 02:05:08 - INFO - utils_multiple_choice -   LOOKING AT ../data/cosmosqa/ directory and train.jsonl file\n",
      "read cosmosqa data: 25262it [00:00, 62232.51it/s]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "label_list = processor.get_labels()\n",
    "if evaluate:\n",
    "    examples = processor.get_dev_examples(args.data_dir, args.eval_file)\n",
    "elif test:\n",
    "    examples = processor.get_test_examples(args.data_dir, args.eval_file)\n",
    "else:\n",
    "    examples = processor.get_train_examples(args.data_dir, args.train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/23/2023 02:13:40 - INFO - __main__ -   Training number: 25262\n",
      "convert examples to features: 0it [00:00, ?it/s]10/23/2023 02:13:40 - INFO - utils_multiple_choice -   Writing example 0 of 25262\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   *** Example ***\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   race_id: 3Q9SPIIRWJKVQ8244310E8TUS6YWAC##34V1S5K3GTZMDUBNBIGY93FLDOB690##A1S1K7134S2VUC##Blog_1044056##q1_a1##3XU9MCX6VQQG7YPLCSAFDPQNH4GR20\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 12350 3470 1771 8 621 226 4832 38 794 258 9 209 8577 307 363 2156 8 51 258 10879 162 409 479 3640 479 2497 3470 1771 16 21979 8 817 162 6675 479 38 269 64 45 244 53 28 1372 77 38 4161 7 106 25606 38 206 24 128 29 5 754 14 51 2551 98 1372 1235 77 51 702 479 2 2 1121 5 499 2156 40 42 621 213 7 192 97 8577 310 17487 9291 9 5 1065 5717 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 12350 3470 1771 8 621 226 4832 38 794 258 9 209 8577 307 363 2156 8 51 258 10879 162 409 479 3640 479 2497 3470 1771 16 21979 8 817 162 6675 479 38 269 64 45 244 53 28 1372 77 38 4161 7 106 25606 38 206 24 128 29 5 754 14 51 2551 98 1372 1235 77 51 702 479 2 2 1121 5 499 2156 40 42 621 213 7 192 97 8577 310 17487 152 621 3829 930 8 3829 7 192 5 311 2156 51 40 192 97 8577 310 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 2\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 12350 3470 1771 8 621 226 4832 38 794 258 9 209 8577 307 363 2156 8 51 258 10879 162 409 479 3640 479 2497 3470 1771 16 21979 8 817 162 6675 479 38 269 64 45 244 53 28 1372 77 38 4161 7 106 25606 38 206 24 128 29 5 754 14 51 2551 98 1372 1235 77 51 702 479 2 2 1121 5 499 2156 40 42 621 213 7 192 97 8577 310 17487 152 621 129 3829 2497 3470 1771 8 18404 226 2156 117 97 8577 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 3\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 12350 3470 1771 8 621 226 4832 38 794 258 9 209 8577 307 363 2156 8 51 258 10879 162 409 479 3640 479 2497 3470 1771 16 21979 8 817 162 6675 479 38 269 64 45 244 53 28 1372 77 38 4161 7 106 25606 38 206 24 128 29 5 754 14 51 2551 98 1372 1235 77 51 702 479 2 2 1121 5 499 2156 40 42 621 213 7 192 97 8577 310 17487 1944 163 8771 16 45 15 2106 8 42 621 64 45 192 106 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   *** Example ***\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   race_id: 3E24UO25QZOMYXHZN4TEH9EMT9GO6L##3UN61F00HXNWYQ7V0G6F8I1FMXXR5I##AO33H4GL9KZX9##Blog_292639##q1_a1##375VMB7D4LXQH9KIJU0NVGG0RV5ID7\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 100 1266 24 4832 45 65 621 26 16005 42965 7 162 479 252 74 33 114 38 222 402 1593 2156 235 17487 125 2156 42 662 2156 38 300 10 486 31 127 2173 23 5 32196 1218 8 37 26 14 38 21 117 1181 956 11 14 737 2156 14 38 222 295 75 240 7 213 66 89 479 2 2 7608 429 33 5 32196 1218 1137 162 38 524 45 956 23 14 737 17487 20 138 5947 5 32196 1138 429 33 56 10 464 9 1508 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 1\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 100 1266 24 4832 45 65 621 26 16005 42965 7 162 479 252 74 33 114 38 222 402 1593 2156 235 17487 125 2156 42 662 2156 38 300 10 486 31 127 2173 23 5 32196 1218 8 37 26 14 38 21 117 1181 956 11 14 737 2156 14 38 222 295 75 240 7 213 66 89 479 2 2 7608 429 33 5 32196 1218 1137 162 38 524 45 956 23 14 737 17487 20 32196 1218 5947 5 32196 1138 429 33 56 10 464 9 1508 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 2\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 100 1266 24 4832 45 65 621 26 16005 42965 7 162 479 252 74 33 114 38 222 402 1593 2156 235 17487 125 2156 42 662 2156 38 300 10 486 31 127 2173 23 5 32196 1218 8 37 26 14 38 21 117 1181 956 11 14 737 2156 14 38 222 295 75 240 7 213 66 89 479 2 2 7608 429 33 5 32196 1218 1137 162 38 524 45 956 23 14 737 17487 9291 9 5 1065 5717 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   choice: 3\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   input_ids: 0 100 1266 24 4832 45 65 621 26 16005 42965 7 162 479 252 74 33 114 38 222 402 1593 2156 235 17487 125 2156 42 662 2156 38 300 10 486 31 127 2173 23 5 32196 1218 8 37 26 14 38 21 117 1181 956 11 14 737 2156 14 38 222 295 75 240 7 213 66 89 479 2 2 7608 429 33 5 32196 1218 1137 162 38 524 45 956 23 14 737 17487 38 429 33 56 10 464 9 1508 447 13 5 138 479 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "10/23/2023 02:13:40 - INFO - utils_multiple_choice -   label: 0\n",
      "convert examples to features: 9988it [00:21, 575.62it/s]10/23/2023 02:14:01 - INFO - utils_multiple_choice -   Writing example 10000 of 25262\n",
      "convert examples to features: 19991it [00:40, 432.49it/s]10/23/2023 02:14:20 - INFO - utils_multiple_choice -   Writing example 20000 of 25262\n",
      "convert examples to features: 25262it [00:51, 492.88it/s]\n",
      "10/23/2023 02:14:31 - INFO - __main__ -   Saving features into cached file ./baselines/cosmosqa-roberta-large/bayes-5e-6-4-8/cached_train_roberta-large_128_cosmosqa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curriculum is False\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Training number: %s\", str(len(examples)))\n",
    "features = convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    args.max_seq_length,\n",
    "    tokenizer,\n",
    "    pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n",
    "    pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    ")\n",
    "if args.local_rank in [-1, 0]:\n",
    "    logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "    torch.save(features, cached_features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    all_input_ids = torch.tensor(select_field(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task_name\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    ")\n",
    "\n",
    "# Other parameters\n",
    "parser.add_argument(\n",
    "    \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    default=128,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\",\n",
    ")\n",
    "parser.add_argument(\"--train_file\", default='', type=str, help=\"Training file.\")\n",
    "parser.add_argument(\"--eval_file\", default='', type=str, help=\"Evaluation file.\")\n",
    "\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_test\", action=\"store_true\", help=\"Whether to run test on the test set\")\n",
    "\n",
    "parser.add_argument(\"--curriculum_learning\", action=\"store_true\", help=\"Whether to use curriculum learning.\")\n",
    "parser.add_argument(\"--starting_percent\", default=0.3, type=float,\n",
    "                    help=\"Starting percentage of training data for curriculum learning\")\n",
    "parser.add_argument(\"--increase_factor\", default=1.1, type=float,\n",
    "                    help=\"Multiplication factor for incrasing data usage after step length iterations\")\n",
    "parser.add_argument(\"--step_length\", default=750, type=int,\n",
    "                    help=\"Number of iterations after which pacing function is updated\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight deay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\n",
    "    \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_steps\",\n",
    "    default=-1,\n",
    "    type=int,\n",
    "    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=50, help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\n",
    "    \"--eval_all_checkpoints\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    ")\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16_opt_level\",\n",
    "    type=str,\n",
    "    default=\"O1\",\n",
    "    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "            \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    ")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
